# Test Scripts Updated

## Changes Made

Both test scripts have been updated to match the GPU fixes in `cybers.py`.

### test_model.py Changes

**1. Force CUDA Device Mapping**
```python
# OLD:
device_map="auto"

# NEW:
device_map_setting = "cuda" if torch.cuda.is_available() else "auto"
device_map=device_map_setting
```

**2. Verify Model Device**
Added check to ensure model actually loaded to GPU:
```python
if torch.cuda.is_available():
    model_device = next(mdl.parameters()).device
    print(f"   âœ… Model is on: {model_device}")
    if str(model_device) == "cpu":
        print("   âš ï¸  WARNING: Model on CPU despite CUDA available!")
```

**3. Explicit Input Device Placement**
```python
# OLD:
inputs = tok([prompt], return_tensors="pt").to(mdl.device)

# NEW:
device = next(mdl.parameters()).device
inputs = tok([prompt], return_tensors="pt")
inputs = {k: v.to(device) for k, v in inputs.items()}
```

**4. Show Device Info During Generation**
```python
print(f"   Model on: {device}")
print(f"   Inputs on: {inputs['input_ids'].device}")
```

### test_client.py

No changes needed - it just makes HTTP requests to the server.

## Why These Changes Matter

Without these fixes, `test_model.py` could show different behavior than the actual server:
- Test might pass on CPU while server fails
- Or test uses GPU while server doesn't
- Inconsistent diagnostics

Now all three files (`cybers.py`, `test_model.py`, and actual server) use the same GPU placement logic.

## Expected Output After Update

When you run `python test_model.py`, you should now see:

```
ğŸ›¡ï¸  CYBER SAFER - MODEL TEST
ğŸ“¦ Model: meta-llama/Llama-3.1-8B-Instruct
ğŸ”§ Quantization: 4-bit
ğŸ–¥ï¸  Device: CUDA (GPU)
ğŸ® GPU: NVIDIA GeForce RTX 5060 Ti
ğŸ’¾ GPU Memory: 16.61 GB

â³ LOADING MODEL (this may take 1-2 minutes)...

1ï¸âƒ£  Loading tokenizer...
   âœ… Tokenizer loaded

2ï¸âƒ£  Loading model (largest step, please wait)...
   ğŸ“ Device map: cuda
   âœ… Model loaded
   âœ… Model is on: cuda:0  â† CRITICAL CHECK!

ğŸ“Š Model has 8.03B parameters

3ï¸âƒ£  TESTING GENERATION

ğŸ’¬ Test prompt: Say 'Hello! The model is working!' and nothing else.
ğŸš€ Generating response...
   Model on: cuda:0
   Inputs on: cuda:0

ğŸ¤– Model response: Hello! The model is working!

âœ… SUCCESS! Model is working correctly.
```

**Key indicators of success:**
- âœ… `Device map: cuda`
- âœ… `Model is on: cuda:0`
- âœ… `Model on: cuda:0`
- âœ… `Inputs on: cuda:0`

If you see "cpu" anywhere, GPU is NOT being used!

## Verification Checklist

After updating, verify:
- [ ] `test_model.py` shows "Model is on: cuda:0"
- [ ] `test_model.py` shows "Inputs on: cuda:0"
- [ ] No warnings about CPU usage
- [ ] Generation completes in seconds (not minutes)
- [ ] `nvidia-smi` shows GPU usage during test

## Files Updated

âœ… `test_model.py` - Updated with GPU fixes
âœ… `test_client.py` - No changes needed (already correct)
âœ… `cybers.py` - Previously updated with GPU fixes

All three now use consistent GPU placement logic!
